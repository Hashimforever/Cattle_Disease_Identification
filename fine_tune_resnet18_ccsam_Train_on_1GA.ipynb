{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HIREE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HIREE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import tqdm\n",
    "import pdb\n",
    "import shutil\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import os.path as osp\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models\n",
    "from functools import partial\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.backends import cudnn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score,precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (ccsam): CCSAM(\n",
      "        (channel_attention): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (spatial_attention): SpatialAttention(\n",
      "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (ccsam): CCSAM(\n",
      "        (channel_attention): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (spatial_attention): SpatialAttention(\n",
      "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (ccsam): CCSAM(\n",
      "        (channel_attention): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (spatial_attention): SpatialAttention(\n",
      "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (ccsam): CCSAM(\n",
      "        (channel_attention): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (spatial_attention): SpatialAttention(\n",
      "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (ccsam): CCSAM(\n",
      "        (channel_attention): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (spatial_attention): SpatialAttention(\n",
      "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (ccsam): CCSAM(\n",
      "        (channel_attention): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (spatial_attention): SpatialAttention(\n",
      "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (ccsam): CCSAM(\n",
      "        (channel_attention): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (spatial_attention): SpatialAttention(\n",
      "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (ccsam): CCSAM(\n",
      "        (channel_attention): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (spatial_attention): SpatialAttention(\n",
      "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (fc): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=256, out_features=5, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#model=Pre_ResNet18Model1(num_classes=5)\n",
    "#model=my_InceptionV3(num_classes=5)\n",
    "model=resnet18_ccsam(num_classes=5)\n",
    "#model=ccsam_inception_v3(num_classes=5)\n",
    "#model = Freezing_layer_Resnet18WithCCSAM1(num_classes=5)\n",
    "#model = model.cuda()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of model parameters: 11396957\n",
      "Number of trainable parameters: 11396957\n",
      "Number of non-trainable parameters: 0\n"
     ]
    }
   ],
   "source": [
    "# get the number of model parameters\n",
    "print('Total Number of model parameters: {}'.format(\n",
    "    sum([p.data.nelement() for p in model.parameters()])))\n",
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "num_non_trainable_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "\n",
    "print(\"Number of trainable parameters:\", num_trainable_params)\n",
    "print(\"Number of non-trainable parameters:\", num_non_trainable_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python DataPreProccessing_1GA.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of training and validation images processing per batches: 7854\n",
      "Total Number of training images processing per batches: 884 99\n",
      "Total Number of images in Testing folder: 2130\n"
     ]
    }
   ],
   "source": [
    "# Data loading\n",
    "\n",
    "train_loader,valid_loader, TestLoader = DataPreProccessing_1GA.preproccessing()\n",
    "# Define the class labels\n",
    "class_labels = ['FMD', 'KCD', 'LD', 'RWD', 'WD']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For this example, we'll be using a cross-entropy loss. For demonstration purposes, we'll create batches of dummy output and label values, run them through the loss function, and examine the result.\n",
    "#criterion = nn.CrossEntropyLoss().cuda()\n",
    "# NB: Loss functions expect data in batches, so we're creating batches of 5\n",
    "# Represents the model's confidence in each of the 5 classes for a given input\n",
    "#dummy_outputs = torch.rand(4, 5)\n",
    "# Represents the correct class among the 5 being tested\n",
    "#class_labels = torch.tensor([1, 2, 3,4])\n",
    "    \n",
    "#print(dummy_outputs)\n",
    "#print(class_labels)\n",
    "\n",
    "#loss = criterion(dummy_outputs, class_labels)\n",
    "#print('Total loss for this batch: {}'.format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up tensorboard writer\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "#filename = 'Pre_ResNet18Model1'\n",
    "#filename = 'my_InceptionV3'\n",
    "filename = 'resnet18_ccsam'\n",
    "#filename = 'ccsam_inception_v3'\n",
    "#filename = 'Freezing_layer_Resnet18WithCCSAM1'\n",
    "log_dir = 'E:/jupyter notebook/last_code_model/Results_log/logsOf_first_GA/'\n",
    "saved_model_path='E:/jupyter notebook/last_code_model/saved_model_code/'\n",
    "writer = SummaryWriter(log_dir + filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 0.271 -- Training Accuracy: 35.144%  -- Validation Loss: 1.422  -- Validation Accuracy: 45.235%\n",
      "Epoch 2: Training Loss: 0.172 -- Training Accuracy: 44.581%  -- Validation Loss: 1.331  -- Validation Accuracy: 47.552%\n",
      "Epoch 3: Training Loss: 0.153 -- Training Accuracy: 52.080%  -- Validation Loss: 1.243  -- Validation Accuracy: 51.971%\n",
      "Epoch 4: Training Loss: 0.132 -- Training Accuracy: 60.385%  -- Validation Loss: 1.148  -- Validation Accuracy: 56.025%\n",
      "Epoch 5: Training Loss: 0.110 -- Training Accuracy: 68.322%  -- Validation Loss: 1.084  -- Validation Accuracy: 58.764%\n",
      "Epoch 6: Training Loss: 0.091 -- Training Accuracy: 73.642%  -- Validation Loss: 0.984  -- Validation Accuracy: 62.815%\n",
      "Epoch 7: Training Loss: 0.075 -- Training Accuracy: 78.919%  -- Validation Loss: 0.900  -- Validation Accuracy: 66.200%\n",
      "Epoch 8: Training Loss: 0.064 -- Training Accuracy: 81.904%  -- Validation Loss: 0.834  -- Validation Accuracy: 68.787%\n",
      "Epoch 9: Training Loss: 0.052 -- Training Accuracy: 85.767%  -- Validation Loss: 0.769  -- Validation Accuracy: 71.322%\n",
      "Epoch 10: Training Loss: 0.046 -- Training Accuracy: 87.634%  -- Validation Loss: 0.710  -- Validation Accuracy: 73.591%\n",
      "Epoch 11: Training Loss: 0.038 -- Training Accuracy: 89.898%  -- Validation Loss: 0.657  -- Validation Accuracy: 75.656%\n",
      "Epoch 12: Training Loss: 0.035 -- Training Accuracy: 90.195%  -- Validation Loss: 0.612  -- Validation Accuracy: 77.452%\n",
      "Epoch 13: Training Loss: 0.030 -- Training Accuracy: 92.105%  -- Validation Loss: 0.571  -- Validation Accuracy: 79.010%\n",
      "Epoch 14: Training Loss: 0.025 -- Training Accuracy: 93.435%  -- Validation Loss: 0.535  -- Validation Accuracy: 80.327%\n",
      "Epoch 15: Training Loss: 0.023 -- Training Accuracy: 94.072%  -- Validation Loss: 0.504  -- Validation Accuracy: 81.528%\n",
      "Epoch 16: Training Loss: 0.020 -- Training Accuracy: 95.317%  -- Validation Loss: 0.475  -- Validation Accuracy: 82.643%\n",
      "Epoch 17: Training Loss: 0.019 -- Training Accuracy: 95.020%  -- Validation Loss: 0.449  -- Validation Accuracy: 83.604%\n",
      "Epoch 18: Training Loss: 0.017 -- Training Accuracy: 95.784%  -- Validation Loss: 0.427  -- Validation Accuracy: 84.458%\n",
      "Epoch 19: Training Loss: 0.016 -- Training Accuracy: 96.194%  -- Validation Loss: 0.407  -- Validation Accuracy: 85.229%\n",
      "Epoch 20: Training Loss: 0.015 -- Training Accuracy: 96.038%  -- Validation Loss: 0.387  -- Validation Accuracy: 85.961%\n",
      "Epoch 21: Training Loss: 0.013 -- Training Accuracy: 96.732%  -- Validation Loss: 0.371  -- Validation Accuracy: 86.539%\n",
      "Epoch 22: Training Loss: 0.013 -- Training Accuracy: 96.859%  -- Validation Loss: 0.355  -- Validation Accuracy: 87.128%\n",
      "Epoch 23: Training Loss: 0.013 -- Training Accuracy: 96.887%  -- Validation Loss: 0.341  -- Validation Accuracy: 87.638%\n",
      "Epoch 24: Training Loss: 0.011 -- Training Accuracy: 97.623%  -- Validation Loss: 0.328  -- Validation Accuracy: 88.126%\n",
      "Epoch 25: Training Loss: 0.010 -- Training Accuracy: 97.482%  -- Validation Loss: 0.316  -- Validation Accuracy: 88.571%\n",
      "Epoch 26: Training Loss: 0.009 -- Training Accuracy: 97.750%  -- Validation Loss: 0.305  -- Validation Accuracy: 88.995%\n",
      "Epoch 27: Training Loss: 0.009 -- Training Accuracy: 98.048%  -- Validation Loss: 0.294  -- Validation Accuracy: 89.375%\n",
      "Epoch 28: Training Loss: 0.008 -- Training Accuracy: 98.161%  -- Validation Loss: 0.285  -- Validation Accuracy: 89.695%\n",
      "Epoch 29: Training Loss: 0.009 -- Training Accuracy: 97.835%  -- Validation Loss: 0.276  -- Validation Accuracy: 90.029%\n",
      "Epoch 30: Training Loss: 0.008 -- Training Accuracy: 98.048%  -- Validation Loss: 0.268  -- Validation Accuracy: 90.348%\n",
      "Epoch 31: Training Loss: 0.007 -- Training Accuracy: 98.345%  -- Validation Loss: 0.260  -- Validation Accuracy: 90.610%\n",
      "Epoch 32: Training Loss: 0.006 -- Training Accuracy: 98.628%  -- Validation Loss: 0.252  -- Validation Accuracy: 90.896%\n",
      "Epoch 33: Training Loss: 0.007 -- Training Accuracy: 98.231%  -- Validation Loss: 0.245  -- Validation Accuracy: 91.164%\n",
      "Epoch 34: Training Loss: 0.006 -- Training Accuracy: 98.769%  -- Validation Loss: 0.239  -- Validation Accuracy: 91.405%\n",
      "Epoch 35: Training Loss: 0.006 -- Training Accuracy: 98.755%  -- Validation Loss: 0.233  -- Validation Accuracy: 91.625%\n",
      "Epoch 36: Training Loss: 0.006 -- Training Accuracy: 98.613%  -- Validation Loss: 0.227  -- Validation Accuracy: 91.844%\n",
      "Epoch 37: Training Loss: 0.004 -- Training Accuracy: 99.179%  -- Validation Loss: 0.221  -- Validation Accuracy: 92.050%\n",
      "Epoch 38: Training Loss: 0.005 -- Training Accuracy: 98.783%  -- Validation Loss: 0.215  -- Validation Accuracy: 92.260%\n",
      "Epoch 39: Training Loss: 0.005 -- Training Accuracy: 98.882%  -- Validation Loss: 0.210  -- Validation Accuracy: 92.442%\n",
      "Epoch 40: Training Loss: 0.005 -- Training Accuracy: 98.840%  -- Validation Loss: 0.205  -- Validation Accuracy: 92.608%\n",
      "Epoch 41: Training Loss: 0.004 -- Training Accuracy: 99.165%  -- Validation Loss: 0.201  -- Validation Accuracy: 92.779%\n",
      "Epoch 42: Training Loss: 0.004 -- Training Accuracy: 99.038%  -- Validation Loss: 0.196  -- Validation Accuracy: 92.945%\n",
      "Epoch 43: Training Loss: 0.004 -- Training Accuracy: 99.151%  -- Validation Loss: 0.192  -- Validation Accuracy: 93.100%\n",
      "Epoch 44: Training Loss: 0.005 -- Training Accuracy: 98.953%  -- Validation Loss: 0.188  -- Validation Accuracy: 93.249%\n",
      "Epoch 45: Training Loss: 0.004 -- Training Accuracy: 98.995%  -- Validation Loss: 0.184  -- Validation Accuracy: 93.399%\n",
      "Epoch 46: Training Loss: 0.003 -- Training Accuracy: 99.321%  -- Validation Loss: 0.180  -- Validation Accuracy: 93.539%\n",
      "Epoch 47: Training Loss: 0.004 -- Training Accuracy: 99.010%  -- Validation Loss: 0.176  -- Validation Accuracy: 93.677%\n",
      "Epoch 48: Training Loss: 0.004 -- Training Accuracy: 99.109%  -- Validation Loss: 0.172  -- Validation Accuracy: 93.806%\n",
      "Epoch 49: Training Loss: 0.002 -- Training Accuracy: 99.547%  -- Validation Loss: 0.169  -- Validation Accuracy: 93.932%\n",
      "Epoch 50: Training Loss: 0.003 -- Training Accuracy: 99.194%  -- Validation Loss: 0.166  -- Validation Accuracy: 94.054%\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "best_vloss = 0.7\n",
    "\n",
    "epoch_number=50\n",
    "# calculate validation loss and accuracy\n",
    "val_loss = 0.0\n",
    "val_correct = 0\n",
    "val_total = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies=[]\n",
    "for epoch in range(epoch_number):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # calculate training accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "         # calculate training loss\n",
    "        avg_trainLoss=running_loss / len(train_loader.dataset)\n",
    "        avg_trainAccuracy= 100 * correct / total\n",
    "    model.eval()\n",
    "    #num_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for i,data in enumerate(valid_loader):\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            val_loss += criterion(outputs, labels).item() * labels.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "        avg_valLoss = val_loss / val_total\n",
    "        avg_Val_Accuracy = 100 * val_correct / (val_total+1)\n",
    "        train_losses.append(loss.item())\n",
    "        val_losses.append(avg_valLoss)\n",
    "        train_accuracies.append(avg_trainAccuracy)\n",
    "        val_accuracies.append(avg_Val_Accuracy)\n",
    "   # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "        global_step = epoch * len(train_loader) + i\n",
    "        writer.add_scalars('loss', {'Training Loss': loss.item(),\n",
    "                                    'Validation Loss': avg_valLoss}, epoch)\n",
    "        writer.add_scalars('Accuracy', {'Training Accuracy': avg_trainAccuracy,\n",
    "                                    'Validation Accuracy': avg_Val_Accuracy}, epoch)\n",
    "        \n",
    "        #Close tensorboard writer\n",
    "        writer.close()\n",
    "    print('Epoch %d: Training Loss: %.3f -- Training Accuracy: %.3f%%  -- Validation Loss: %.3f  -- Validation Accuracy: %.3f%%' %\n",
    "          (epoch + 1, avg_trainLoss, avg_trainAccuracy, avg_valLoss, avg_Val_Accuracy))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_valLoss < best_vloss:\n",
    "        best_vloss = avg_valLoss\n",
    "        model_path = os.path.join(saved_model_path, filename + '.pth')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    epoch_number += 1\n",
    "\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(val_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "# Plot the training and validation accuracy\n",
    "plt.plot(train_accuracies, label='train_Accuracy')\n",
    "plt.plot(val_accuracies, label='val_Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(frameon=False)\n",
    "plt.show()\n",
    "plt.plot(train_accuracies, label='train_Accuracy')\n",
    "plt.plot(val_accuracies, label='val_Accuracy')\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(val_losses, label='Validation loss')\n",
    "plt.ylabel('accuracy vs loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 99.34\n",
      "Confusion matrix:\n",
      "[[ 99.6          0.           0.           0.2          0.2       ]\n",
      " [  0.         100.           0.           0.           0.        ]\n",
      " [  0.           0.22321429  99.77678571   0.           0.        ]\n",
      " [  0.           0.25         0.          98.5          1.25      ]\n",
      " [  0.33112583   0.           0.           1.32450331  98.34437086]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       500\n",
      "         1.0       1.00      1.00      1.00       480\n",
      "         2.0       1.00      1.00      1.00       448\n",
      "         3.0       0.99      0.98      0.99       400\n",
      "         4.0       0.98      0.98      0.98       302\n",
      "\n",
      "    accuracy                           0.99      2130\n",
      "   macro avg       0.99      0.99      0.99      2130\n",
      "weighted avg       0.99      0.99      0.99      2130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score,precision_recall_fscore_support\n",
    "\n",
    "model_path ='E:/jupyter notebook/last_code_model/saved_model_code/resnet18_ccsam.pth'\n",
    "# Load saved model\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data, labels in TestLoader:\n",
    "        data=data \n",
    "        labels =  labels\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Load saved model\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define empty arrays for storing true labels and predicted labels\n",
    "true_labels = np.array([])\n",
    "pred_labels = np.array([])\n",
    "\n",
    "# Define empty arrays for storing loss and accuracy\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "# Evaluate model on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data, labels in TestLoader:\n",
    "        data = data\n",
    "        labels = labels\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Get predictions and calculate accuracy\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        true_labels = np.append(true_labels, labels.cpu().numpy())\n",
    "        pred_labels = np.append(pred_labels, preds.cpu().numpy())\n",
    "        accuracy = accuracy_score(true_labels, pred_labels) * 100\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_mat = confusion_matrix(true_labels, pred_labels)\n",
    "# Normalize confusion matrix to percentages\n",
    "conf_mat_pct = 100 * conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Print confusion matrix\n",
    "print('Confusion matrix:')\n",
    "print(conf_mat_pct)\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(conf_mat_pct, cmap='Blues')\n",
    "\n",
    "# Add color bar\n",
    "cbar = ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "# Show all ticks and label them with the respective list entries\n",
    "ax.set_xticks(np.arange(len(class_labels)))\n",
    "ax.set_yticks(np.arange(len(class_labels)))\n",
    "ax.set_xticklabels(class_labels)\n",
    "ax.set_yticklabels(class_labels)\n",
    "\n",
    "# Set axis labels\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "\n",
    "# Rotate the tick labels and set their alignment\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n",
    "\n",
    "# Loop over data dimensions and create text annotations\n",
    "for i in range(len(class_labels)):\n",
    "    for j in range(len(class_labels)):\n",
    "        ax.text(j, i, format(conf_mat_pct[i, j], '.2f') + '%',\n",
    "                ha='center', va='center', color='white' if conf_mat[i, j] > conf_mat.max() / 2 else 'black')\n",
    "\n",
    "# Set title\n",
    "ax.set_title('Confusion matrix')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "# classification report\n",
    "print(classification_report(true_labels, pred_labels))\n",
    "\n",
    "# accuracy score\n",
    "acc = accuracy_score(true_labels, pred_labels)\n",
    "\n",
    "# precision, recall, f1-score, support\n",
    "precision, recall, f1score, support = precision_recall_fscore_support(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class:  0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_path = os.path.join(saved_model_path, filename + '.pth')\n",
    "# Load saved model\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Load single test image and preprocess it\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "img_path = \"C:/Users/yared/Desktop/hashim file/Research Data/processed_10000_kan_sirrii_SecondGA_my/test/Foot_mouth_disease/FMD11 (34).jpg\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "img = Image.open(img_path)\n",
    "img_tensor = transform(img)\n",
    "img_tensor = img_tensor.unsqueeze(0) # add batch dimension\n",
    "\n",
    "# Test model on single image\n",
    "with torch.no_grad():\n",
    "    outputs = model(img_tensor)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "print('Predicted class: ', predicted.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [512, 256, 3, 3], expected input[1, 3, 224, 224] to have 256 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m pooled_gradients \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(gradients, dim\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Get the feature maps from the last convolutional layer\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer4\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mresize(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)\n\u001b[0;32m     60\u001b[0m feature_hook \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mregister_forward_hook(features)\n\u001b[0;32m     61\u001b[0m gradient_hook \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mregister_backward_hook(pooled_gradients)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mE:\\jupyter notebook\\last_code_model\\model\\modified_model.py:32\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     31\u001b[0m     residual \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m---> 32\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[0;32m     34\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [512, 256, 3, 3], expected input[1, 3, 224, 224] to have 256 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "from torchvision.models import ResNet\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Load the pre-trained RESNET18 model with CBAM attention mechanism\n",
    "model=resnet18_ccsam(num_classes=5)\n",
    "\n",
    "# Remove the last fully connected layer\n",
    "#model.fc = torch.nn.Identity()\n",
    "#model.fc2 = torch.nn.Identity()\n",
    "#model = Freezing_layer_Resnet18WithCCSAM1(num_classes=5)\n",
    "\n",
    "# Load the ResNet18 model\n",
    "#model = models.resnet18(pretrained=True)\n",
    "#model.eval()\n",
    "# Load the pre-trained ResNet50 model\n",
    "#model=Pre_ResNet18Model1(num_classes=5)\n",
    "model.eval()\n",
    "\n",
    "# Define the preprocessing transformations\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_path = \"E:/jupyter notebook/my_trying_fie/All_In_One/DatasetContainer/Processed_10000_FirstGA_other/test/FMD/FMD_0_3215.jpg\"\n",
    "#image_path = \"E:/jupyter notebook/my_trying_fie/All_In_One/DatasetContainer/Processed_10000_FirstGA_other/test/KCD/KCJ18.jpg\"\n",
    "#image_path = \"E:/jupyter notebook/my_trying_fie/All_In_One/DatasetContainer/Processed_10000_FirstGA_other/test/LD/LD (24).jpg\"\n",
    "#image_path = \"E:/jupyter notebook/my_trying_fie/All_In_One/DatasetContainer/Processed_10000_FirstGA_other/test/RWD/RD (1)_0_9520.jpg\"\n",
    "#image_path = \"E:/jupyter notebook/my_trying_fie/All_In_One/DatasetContainer/Processed_10000_FirstGA_other/test/WD/WD1 (5).jpg\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "# Preprocess the image\n",
    "\n",
    "input_tensor = preprocess(image).unsqueeze(0)\n",
    "\n",
    "# Forward pass through the model\n",
    "output = model(input_tensor)\n",
    "pred_label = torch.argmax(output, dim=1).item()\n",
    "\n",
    "# Get the gradients with respect to the input tensor\n",
    "model.zero_grad()\n",
    "output[:, pred_label].backward()\n",
    "\n",
    "# Get the gradients from the model's first convolutional layer\n",
    "gradients = model.conv1.weight.grad\n",
    "\n",
    "# Average the gradients across the RGB channels\n",
    "pooled_gradients = torch.mean(gradients, dim=[2, 3])\n",
    "\n",
    "# Get the feature maps from the last convolutional layer\n",
    "features = model.layer4(input_tensor)\n",
    "\n",
    "feature_hook = model.register_forward_hook(features)\n",
    "gradient_hook = model.register_backward_hook(pooled_gradients)\n",
    "# Multiply the feature maps by the corresponding gradients\n",
    "for i in range(features.size(0)):\n",
    "    features[:, i, :, :] *= pooled_gradients[0, i]\n",
    "\n",
    "# Sum the feature maps along the channels dimension\n",
    "heatmap = torch.sum(features, dim=1).squeeze()\n",
    "\n",
    "# Normalize the heat map values\n",
    "heatmap = nn.functional.relu(heatmap)\n",
    "heatmap /= torch.max(heatmap)\n",
    "\n",
    "# Convert the heat map to a numpy array\n",
    "heatmap = heatmap.detach().numpy()\n",
    "feature_hook.remove()\n",
    "gradient_hook.remove()\n",
    "# Visualize the heat map\n",
    "#plt.imshow('Original Image', image)\n",
    "#plt.imshow(heatmap, cmap='jet')\n",
    "#plt.axis('off')\n",
    "#plt.show()\n",
    "\n",
    "# Apply the attention map to the original image\n",
    "#heatmap = cv2.applyColorMap(attention_map, cv2.COLORMAP_JET)\n",
    "\n",
    "# Display the original image, attention map, and superimposed image\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].imshow(image)\n",
    "ax[0].set_title('Original')\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(heatmap, cmap='jet')\n",
    "ax[1].set_title('Resnet18')\n",
    "ax[1].axis('off')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 512]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[148], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m output \u001b[38;5;241m=\u001b[39m model(input_batch)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Get the activation maps from the last convolutional layer\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m activation_maps \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer4\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m activation_maps \u001b[38;5;241m=\u001b[39m activation_maps\u001b[38;5;241m.\u001b[39munsqueeze()\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Convert the activation maps to numpy array\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 512]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Load the pre-trained RESNET18 model with CBAM attention mechanism\n",
    "model=resnet18_ccsam(num_classes=5)\n",
    "\n",
    "# Remove the last fully connected layer\n",
    "model.fc = torch.nn.Identity()\n",
    "model.fc2 = torch.nn.Identity()\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define the transform to preprocess the image\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the image\n",
    "image_path = 'E:/jupyter notebook/my_trying_fie/All_In_One/DatasetContainer/Processed_10000_FirstGA_other/test/FMD/FMD_0_4290.jpg'\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "# Preprocess the image\n",
    "input_tensor = preprocess(image)\n",
    "input_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "# Forward pass through the model\n",
    "output = model(input_batch)\n",
    "\n",
    "# Get the activation maps from the last convolutional layer\n",
    "activation_maps = model.layer4[-1].conv2(output)\n",
    "activation_maps = activation_maps.unsqueeze()\n",
    "# Convert the activation maps to numpy array\n",
    "activation_maps = activation_maps.detach().cpu().numpy()\n",
    "\n",
    "# Calculate the mean across the channel dimension\n",
    "mean_activation_map = np.mean(activation_maps, axis=1)\n",
    "\n",
    "# Normalize the mean activation map between 0 and 1\n",
    "mean_activation_map = (mean_activation_map - np.min(mean_activation_map)) / (\n",
    "    np.max(mean_activation_map) - np.min(mean_activation_map)\n",
    ")\n",
    "\n",
    "# Resize the mean activation map to match the image size\n",
    "mean_activation_map = cv2.resize(\n",
    "    mean_activation_map, (image.size[0], image.size[1])\n",
    ")\n",
    "\n",
    "# Apply the mean activation map as the kernel activation map\n",
    "kernel_activation_map = mean_activation_map\n",
    "\n",
    "# Convert the image to numpy array\n",
    "image = np.array(image)\n",
    "\n",
    "# Scale the kernel activation map to the range of 0 to 255\n",
    "kernel_activation_map = cv2.convertScaleAbs(\n",
    "    kernel_activation_map * 255\n",
    ")\n",
    "\n",
    "# Apply the kernel activation map to the image\n",
    "heatmap = cv2.applyColorMap(\n",
    "    kernel_activation_map, cv2.COLORMAP_JET\n",
    ")\n",
    "\n",
    "# Superimpose the kernel activation map on the image\n",
    "superimposed_img = cv2.addWeighted(\n",
    "    image, 0.6, heatmap, 0.4, 0\n",
    ")\n",
    "\n",
    "# Display the original image, kernel activation map, and the superimposed image\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.imshow('Kernel Activation Map', kernel_activation_map)\n",
    "cv2.imshow('Superimposed Image', superimposed_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (ccsam): CCSAM(\n",
      "        (channel_attention): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (spatial_attention): SpatialAttention(\n",
      "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (ccsam): CCSAM(\n",
      "        (channel_attention): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (spatial_attention): SpatialAttention(\n",
      "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (ccsam): CCSAM(\n",
      "        (channel_attention): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (spatial_attention): SpatialAttention(\n",
      "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (ccsam): CCSAM(\n",
      "        (channel_attention): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (spatial_attention): SpatialAttention(\n",
      "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (ccsam): CCSAM(\n",
      "        (channel_attention): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (spatial_attention): SpatialAttention(\n",
      "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (ccsam): CCSAM(\n",
      "        (channel_attention): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (spatial_attention): SpatialAttention(\n",
      "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (ccsam): CCSAM(\n",
      "        (channel_attention): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (spatial_attention): SpatialAttention(\n",
      "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (ccsam): CCSAM(\n",
      "        (channel_attention): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (spatial_attention): SpatialAttention(\n",
      "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (fc): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=256, out_features=5, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#model=Pre_ResNet18Model1(num_classes=5)\n",
    "#model=my_InceptionV3(num_classes=5)\n",
    "model=resnet18_ccsam(num_classes=5)\n",
    "#model=ccsam_inception_v3(num_classes=5)\n",
    "#model = Freezing_layer_Resnet18WithCCSAM1(num_classes=5)\n",
    "#model = model.cuda()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [512, 256, 3, 3], expected input[1, 3, 224, 224] to have 256 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 106\u001b[0m\n\u001b[0;32m    103\u001b[0m target_class_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# Replace with the target class index for visualization\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Generate the heatmap\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m heatmap \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_cam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_heatmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_class_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# Resize the heatmap to match the input image size\u001b[39;00m\n\u001b[0;32m    109\u001b[0m heatmap \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39muint8(heatmap \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\n",
      "Cell \u001b[1;32mIn[23], line 73\u001b[0m, in \u001b[0;36mGradCAM.generate_heatmap\u001b[1;34m(self, input_image, target_class)\u001b[0m\n\u001b[0;32m     70\u001b[0m pooled_gradients \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(gradients, dim\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Get the feature maps from the last convolutional layer\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer4\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Multiply the feature maps by the corresponding gradients\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(features\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1535\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1536\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1538\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1540\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   1541\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1542\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1543\u001b[0m     ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\resnet.py:92\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m     90\u001b[0m     identity \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m---> 92\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[0;32m     94\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [512, 256, 3, 3], expected input[1, 3, 224, 224] to have 256 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from model.ccsam_module import CCSAM\n",
    "# Load the pre-trained ResNet-18 model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Modify the ResNet-18 model to include CBAM\n",
    "model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "model.fc = nn.Linear(512, 10)  # Modify the last fully connected layer for your task\n",
    "model.layer1[-1].relu = CCSAM(64)\n",
    "model.layer2[-1].relu = CCSAM(128)\n",
    "model.layer3[-1].relu = CCSAM(256)\n",
    "model.layer4[-1].relu = CCSAM(512)\n",
    "# Define the Grad-CAM class\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradient = None\n",
    "        self.activations = None\n",
    "\n",
    "        self.model.eval()\n",
    "        self.register_hooks()\n",
    "\n",
    "    def register_hooks(self):\n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            self.gradient = grad_output[0]\n",
    "\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output\n",
    "\n",
    "        target_layer = self.target_layer\n",
    "        for name, module in self.model.named_modules():\n",
    "            if name == target_layer:\n",
    "                module.register_backward_hook(backward_hook)\n",
    "                module.register_forward_hook(forward_hook)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def backward(self, index=None):\n",
    "        if index is None:\n",
    "            index = torch.argmax(self.output)\n",
    "\n",
    "        # Calculate the sum of self.output for scalar output\n",
    "        sum_output = self.output.sum()\n",
    "        sum_output.backward(retain_graph=True)\n",
    "\n",
    "    def generate_heatmap(self, input_image, target_class=None):\n",
    "       \n",
    "        # Forward pass through the model\n",
    "        self.output = self.forward(input_image)\n",
    "        pred_label = torch.argmax(self.output, dim=1).item()\n",
    "\n",
    "        # Get the gradients with respect to the input tensor\n",
    "        self.model.zero_grad()\n",
    "        self.output[:, pred_label].backward()\n",
    "\n",
    "        # Get the gradients from the model's first convolutional layer\n",
    "        gradients = self.model.conv1.weight.grad\n",
    "\n",
    "        # Average the gradients across the RGB channels\n",
    "        pooled_gradients = torch.mean(gradients, dim=[2, 3])\n",
    "\n",
    "        # Get the feature maps from the last convolutional layer\n",
    "        features = self.model.layer4(input_image)\n",
    "\n",
    "        # Multiply the feature maps by the corresponding gradients\n",
    "        for i in range(features.size(0)):\n",
    "            features[:, i, :, :] *= pooled_gradients[0, i]\n",
    "\n",
    "        # Sum the feature maps along the channels dimension\n",
    "        heatmap = torch.sum(features, dim=1).squeeze()\n",
    "\n",
    "        # Normalize the heat map values\n",
    "        heatmap = nn.functional.relu(heatmap)\n",
    "        heatmap /= torch.max(heatmap)\n",
    "\n",
    "        return heatmap.squeeze().detach().numpy()\n",
    "\n",
    "\n",
    "# Load the image and preprocess\n",
    "image_path = 'E:/jupyter notebook/my_trying_fie/All_In_One/DatasetContainer/Processed_10000_FirstGA_other/test/FMD/FMD_0_4290.jpg'  # Replace with your image path\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_image = preprocess(image).unsqueeze(0)\n",
    "\n",
    "# Initialize the Grad-CAM object\n",
    "grad_cam = GradCAM(model, target_layer='layer4')\n",
    "\n",
    "# Choose the target class index\n",
    "target_class_index = 5  # Replace with the target class index for visualization\n",
    "\n",
    "# Generate the heatmap\n",
    "heatmap = grad_cam.generate_heatmap(input_image, target_class=target_class_index)\n",
    "\n",
    "# Resize the heatmap to match the input image size\n",
    "heatmap = np.uint8(heatmap * 255)\n",
    "heatmap = np.uint8(Image.fromarray(heatmap).resize((224, 224), Image.BILINEAR))\n",
    "\n",
    "# Resize the input image to match the heatmap size\n",
    "resized_image = image.resize((224, 224))\n",
    "\n",
    "# Apply the heatmap on the input image\n",
    "heatmap_expanded = np.expand_dims(heatmap, axis=2)  # Expand the dimensions of the heatmap\n",
    "blended_image = np.float32(resized_image) + np.float32(heatmap_expanded)\n",
    "blended_image /= np.max(blended_image)\n",
    "\n",
    "# Plot the heatmap overlay and the original image\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.imshow(resized_image)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Original Image')\n",
    "\n",
    "ax2.imshow(blended_image)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Heatmap Overlay')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
